{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains a solution for the Continuous Control project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893). This solution was prepared on local machine under Ubuntu-20.04. Instructions for environment setup are located in README.md file.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if not already done, fetch environment files and start it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# fetch the Reacher Linux environment if not present\n",
    "![[ ! -d \"Reacher_Linux\" ]] && wget https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip && unzip Reacher_Linux.zip\n",
    "\n",
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or switch to reacher one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "![[ ! -d \"Reacher_Linux_one\" ]] && wget -O Reacher_Linux_one.zip https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux.zip && unzip Reacher_Linux_one.zip -d Reacher_Linux_one\n",
    "\n",
    "env = UnityEnvironment(file_name='Reacher_Linux_one/Reacher_Linux/Reacher.x86_64')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A2C\n",
    "\n",
    "DQN learing algorithm and model is contained in the Agent class located in dqn_agent.py. The cell below initializes learning agent and prints model structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.state_encoding = nn.Sequential(\n",
    "            nn.Linear(state_size, fc1_units),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(fc1_units),\n",
    "            nn.Linear(fc1_units, fc2_units),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(fc2_units, action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.variance = nn.Sequential(\n",
    "            nn.Linear(fc2_units, action_size),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\" Action is sampled from normal distribution mean and variance \"\"\"\n",
    "        x = self.state_encoding(x)\n",
    "        return self.mean(x), self.variance(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, seed, fc1_units=128, fc2_units=64):\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.state_encoding = nn.Sequential(\n",
    "            nn.Linear(state_size, fc1_units),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(fc1_units),\n",
    "            nn.Linear(fc1_units, fc2_units),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(fc2_units, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a critic (value) network that maps (state) -> V-values.\"\"\"\n",
    "        x = self.state_encoding(state)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (state_encoding): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (mean): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=4, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (variance): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=4, bias=True)\n",
      "    (1): Softplus(beta=1, threshold=20)\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (state_encoding): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# from model import Actor, Critic\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "GAMMA = 0.9             # discount factor\n",
    "LR_ACTOR = 5e-3         # learning rate of the actor \n",
    "LR_CRITIC = 5e-3        # learning rate of the critic\n",
    "ENTROPY = 1e-3          # strength of the entropy regularization term\n",
    "CLIPPING = 5            # gradient clipping coefficient\n",
    "# UPDATE_EVERY = 5        # action interval at which model is updated\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, device):#, update_every=UPDATE_EVERY):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.device = device\n",
    "        # self.update_every = update_every\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = RolloutBuffer()\n",
    "\n",
    "        # Actor Network\n",
    "        self.policy = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.policy_optimizer = optim.RMSprop(self.policy.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network\n",
    "        self.state_value = Critic(state_size, random_seed).to(device)\n",
    "        self.state_value_optimizer = optim.RMSprop(self.state_value.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "    def step(self, state, actions, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / rewards\n",
    "        self.memory.add(state, actions, reward, next_state, done)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        mean = torch.zeros(state.shape[0], action_size)\n",
    "        variance = torch.zeros(state.shape[0], action_size)\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            for worker in range(state.shape[0]):\n",
    "                worker_state = state[worker].unsqueeze(0)\n",
    "                mean[worker], variance[worker] = self.policy(worker_state)\n",
    "        self.policy.train()\n",
    "\n",
    "        sigma = torch.sqrt(variance)\n",
    "        policy = Normal(mean, sigma)\n",
    "\n",
    "        return policy.sample()\n",
    "\n",
    "    def discounted_returns(self, rewards, dones, V_last):\n",
    "        G = torch.zeros_like(rewards)\n",
    "        G[-1] = V_last * (1 - dones[-1])\n",
    "\n",
    "        for i in reversed(range(len(rewards) - 1)):\n",
    "            G[i] = rewards[i] + GAMMA * G[i - 1]\n",
    "\n",
    "        return G\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        e = self.memory.sample()\n",
    "\n",
    "        e_states = torch.FloatTensor(e.state).to(device)\n",
    "        e_rewards = torch.FloatTensor(e.reward).to(device)\n",
    "        e_actions = torch.stack(e.action).to(device)\n",
    "        e_last_states = torch.FloatTensor(e.next_state[-1]).to(device)\n",
    "        e_dones = torch.FloatTensor(e.done).to(device)\n",
    "\n",
    "        for worker in range(num_agents):\n",
    "            states = e_states[:, worker]\n",
    "            rewards = e_rewards[:, worker]\n",
    "            actions = e_actions[:, worker]\n",
    "            last_states = e_last_states[worker]\n",
    "            dones = e_dones[:, worker]\n",
    "\n",
    "\n",
    "            # ---------------------------- update critic ---------------------------- #\n",
    "            # Compute V targets for current states (y_i)\n",
    "            # Compute critic loss\n",
    "            V_expected = self.state_value(states)\n",
    "\n",
    "            self.state_value.eval()\n",
    "            with torch.no_grad():\n",
    "                V_next_expected = self.state_value(last_states.unsqueeze(0))\n",
    "            self.state_value.train()\n",
    "\n",
    "            R = self.discounted_returns(rewards, dones, V_next_expected).to(device)\n",
    "        \n",
    "            critic_loss = 0.5 * F.mse_loss(V_expected, R)\n",
    "            # Minimize the loss\n",
    "            self.state_value_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.state_value.parameters(), CLIPPING)\n",
    "            self.state_value_optimizer.step()\n",
    "\n",
    "            # ---------------------------- update actor ---------------------------- #\n",
    "            # Compute actor loss\n",
    "\n",
    "            mean, variance = self.policy(states)\n",
    "            sigma = torch.sqrt(variance)\n",
    "            policy_distro = Normal(mean, sigma)\n",
    "            logprobs = policy_distro.log_prob(actions)\n",
    "            entropy = policy_distro.entropy()\n",
    "\n",
    "            # A(s,a) = Q(s,a) - V(s)\n",
    "            actor_loss = (-(R - V_expected.detach()) * logprobs + ENTROPY * entropy)\n",
    "                \n",
    "            w_actor_loss = actor_loss.mean()\n",
    "            # Minimize the loss\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            w_actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), CLIPPING)\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, seed):\n",
    "        self.memory = list()\n",
    "        field_names=[\n",
    "            \"state\", \"action\", \"reward\", \"next_state\", \"done\"\n",
    "        ]\n",
    "        self.experience = namedtuple(\"Experience\", field_names=field_names)\n",
    "        self.fields_len = len(field_names)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(\n",
    "            state,\n",
    "            action,\n",
    "            np.expand_dims(reward, 1),\n",
    "            next_state,\n",
    "            np.expand_dims(done, 1)\n",
    "        )\n",
    "\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Consume all rollout of experiences from memory.\"\"\"\n",
    "        experiences = self.experience(*zip(*self.memory))\n",
    "        \n",
    "        self.memory.clear()\n",
    "        return experiences\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=0, device=device)\n",
    "print(agent.policy)\n",
    "print(agent.state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = agent.act(states)       # select an action\n",
    "\n",
    "env_info = env.step(actions.detach().cpu().numpy())[brain_name]       # send the action to the environment\n",
    "next_states = env_info.vector_observations     # get the next state\n",
    "rewards = env_info.rewards                     # get the reward\n",
    "dones = env_info.local_done                    # see if episode has finished\n",
    "agent.step(states, actions, rewards, next_states, dones)\n",
    "states = next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=(array([[ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  7.90150833e+00,\n",
       "        -1.00000000e+00,  1.25147629e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -1.29508138e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.38918686e+00,\n",
       "        -1.00000000e+00,  7.87846375e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  7.61166334e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -7.98903656e+00,\n",
       "        -1.00000000e+00,  4.18689728e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  3.54782343e-02],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.47213554e+00,\n",
       "        -1.00000000e+00,  7.60845232e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  4.68097657e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.11338353e+00,\n",
       "        -1.00000000e+00, -7.92214489e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -4.35696244e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -7.41746998e+00,\n",
       "        -1.00000000e+00,  2.99685478e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -6.28730536e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -4.12030411e+00,\n",
       "        -1.00000000e+00, -6.85733795e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  9.74787235e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -5.24847221e+00,\n",
       "        -1.00000000e+00,  6.03767586e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  5.67709208e-02],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -5.03456116e+00,\n",
       "        -1.00000000e+00,  6.21716881e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  3.16133469e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  6.55321503e+00,\n",
       "        -1.00000000e+00, -4.58861160e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  4.18611974e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.93537521e+00,\n",
       "        -1.00000000e+00, -7.76236725e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -7.18573332e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  6.03767395e+00,\n",
       "        -1.00000000e+00, -5.24847412e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.95290446e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  5.14229965e+00,\n",
       "        -1.00000000e+00, -6.12835503e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  2.29530931e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  7.96955776e+00,\n",
       "        -1.00000000e+00,  6.97246552e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.59579086e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -7.99878168e+00,\n",
       "        -1.00000000e+00,  1.39617920e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -5.87672472e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -6.97242737e-01,\n",
       "        -1.00000000e+00,  7.96955776e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -3.96821141e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -7.46864414e+00,\n",
       "        -1.00000000e+00, -2.86694336e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  5.95939159e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  6.03767395e+00,\n",
       "        -1.00000000e+00, -5.24847412e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -5.23933172e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.50696898e+00,\n",
       "        -1.00000000e+00, -7.19035339e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  5.68796635e-01],\n",
       "       [ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -4.37113883e-08,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.00000000e+01,\n",
       "         0.00000000e+00,  1.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.20510101e+00,\n",
       "        -1.00000000e+00, -7.69009399e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.31489396e-01]]),), action=(tensor([[-1.3252, -0.7955,  0.0657,  0.0090],\n",
       "        [ 1.0694, -0.2787, -0.1234, -0.1585],\n",
       "        [-1.5603,  0.9486, -1.3372, -0.5115],\n",
       "        [-0.7351, -0.8653,  0.7898, -0.0168],\n",
       "        [-0.3646, -0.9886, -0.6863,  0.4596],\n",
       "        [-1.2429,  0.4147,  0.2903,  0.5701],\n",
       "        [-0.2059,  0.2295,  0.3115, -0.8028],\n",
       "        [ 0.1759,  0.9479, -0.5869,  0.7621],\n",
       "        [-1.1970,  1.1437, -1.0777, -0.1741],\n",
       "        [ 0.0059, -0.2301, -0.7636,  1.3461],\n",
       "        [-0.4603, -0.7655,  0.0026,  1.2742],\n",
       "        [-1.6710, -1.3154, -0.9088, -0.3428],\n",
       "        [-0.5563,  0.6334,  0.7171, -0.4047],\n",
       "        [ 0.5110, -0.1927,  0.4887,  1.3778],\n",
       "        [-0.2118,  0.1950,  0.3522,  0.2732],\n",
       "        [-0.1525,  0.3059, -0.4621, -0.4989],\n",
       "        [-0.3382, -0.2709, -0.3977,  0.6284],\n",
       "        [ 0.0993, -0.7507,  0.5841, -0.4442],\n",
       "        [-0.8435, -0.6047, -0.8122, -0.7855],\n",
       "        [-1.2840,  1.7773, -1.6272, -0.0240]], device='cuda:0'),), reward=(array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]]),), next_state=(array([[-9.53598022e-02, -3.99721456e+00,  1.28940582e-01,\n",
       "         9.99801040e-01, -1.18597923e-02,  1.65616948e-04,\n",
       "         1.60404555e-02, -5.77609062e-01, -8.78665596e-03,\n",
       "        -4.27625507e-01, -1.71933258e+00,  7.37880841e-02,\n",
       "         2.32084775e+00, -6.95838928e-02, -9.99145317e+00,\n",
       "         9.13391113e-02,  9.99620080e-01,  1.61408726e-02,\n",
       "         1.10393122e-03, -2.23171934e-02,  8.02193284e-01,\n",
       "        -5.85607626e-02,  5.75869679e-01, -4.30799872e-01,\n",
       "         2.84420490e-01,  4.86353040e-01,  7.88986969e+00,\n",
       "        -1.00000000e+00,  1.32286453e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -1.29508138e-01],\n",
       "       [-6.74819946e-03, -3.99886870e+00, -1.03725433e-01,\n",
       "         9.99916434e-01, -8.39097891e-04, -9.15609417e-06,\n",
       "        -1.29017225e-02,  5.13590574e-01,  5.06058277e-04,\n",
       "        -3.25678885e-02, -1.30942613e-01,  3.22122760e-02,\n",
       "        -2.06444407e+00, -1.70173645e-02, -9.99643612e+00,\n",
       "        -6.96105957e-02,  9.99826849e-01, -8.73156940e-04,\n",
       "        -3.14567878e-05,  1.85878538e-02, -7.38944769e-01,\n",
       "         1.70653232e-03, -3.65606137e-02, -3.91321182e-01,\n",
       "         1.30177066e-01, -3.13911527e-01,  1.80568314e+00,\n",
       "        -1.00000000e+00,  7.79355621e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  7.61166334e-01],\n",
       "       [ 1.46743774e-01, -3.99736142e+00, -5.79319000e-02,\n",
       "         9.99807417e-01,  1.82524398e-02,  1.15204813e-04,\n",
       "        -7.20778294e-03,  2.28617534e-01, -7.10914843e-03,\n",
       "         7.20620513e-01,  2.89652300e+00,  7.31376484e-02,\n",
       "        -9.18202937e-01,  7.36885071e-02, -9.98776913e+00,\n",
       "         5.52062988e-02,  9.99195576e-01, -3.04001831e-02,\n",
       "         1.72100565e-03,  2.60979962e-02, -9.06037688e-01,\n",
       "        -9.80217084e-02, -1.20101309e+00, -3.01876009e-01,\n",
       "         4.62435246e-01,  2.41273332e+00, -7.98797607e+00,\n",
       "        -1.00000000e+00,  4.38482285e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  3.54782343e-02],\n",
       "       [-1.37701035e-01, -3.99040246e+00,  2.59914905e-01,\n",
       "         9.99329925e-01, -1.71205550e-02,  4.94571635e-04,\n",
       "         3.23466398e-02, -1.08114004e+00, -2.40873061e-02,\n",
       "        -5.56874633e-01, -2.24059749e+00,  2.39687830e-01,\n",
       "         4.33962393e+00, -7.84683228e-02, -9.96080494e+00,\n",
       "         8.87382850e-02,  9.97768760e-01,  2.68291906e-02,\n",
       "         4.32717148e-03, -6.09837696e-02,  2.02974939e+00,\n",
       "        -2.09331900e-01,  8.22790027e-01, -1.82838321e-01,\n",
       "         1.29331601e+00, -1.40032613e+00,  2.71941185e+00,\n",
       "        -1.00000000e+00,  7.52361631e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  4.68097657e-01],\n",
       "       [-1.57986075e-01, -3.99719071e+00, -3.91642414e-02,\n",
       "         9.99795079e-01, -1.96514577e-02, -8.00803246e-05,\n",
       "        -4.87272814e-03,  2.09868237e-01,  4.17179428e-03,\n",
       "        -7.50499189e-01, -3.01637840e+00,  7.80555308e-02,\n",
       "        -8.43060613e-01, -8.38823169e-02, -9.98929691e+00,\n",
       "         2.08270885e-02,  9.99376595e-01,  3.19927707e-02,\n",
       "        -1.05602341e-03,  1.48908496e-02, -6.19114339e-01,\n",
       "         5.89171872e-02,  1.22551012e+00,  1.92794472e-01,\n",
       "         4.00106430e-01,  1.27350640e+00,  1.35380459e+00,\n",
       "        -1.00000000e+00, -7.88461876e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -4.35696244e-01],\n",
       "       [-7.03659058e-02, -3.99755287e+00,  1.33537292e-01,\n",
       "         9.99823689e-01, -8.75115674e-03,  1.28142347e-04,\n",
       "         1.66115221e-02, -6.40469432e-01, -5.52575476e-03,\n",
       "        -2.70372540e-01, -1.08711016e+00,  6.57714233e-02,\n",
       "         2.57384658e+00,  1.99699402e-02, -9.98951435e+00,\n",
       "         7.34844208e-02,  9.99361157e-01,  2.37901676e-02,\n",
       "         1.23194698e-03, -2.66422257e-02,  1.01735973e+00,\n",
       "        -5.99588230e-02,  7.67550468e-01,  1.52677155e+00,\n",
       "         3.74582767e-01, -2.50556674e-02, -7.54182625e+00,\n",
       "        -1.00000000e+00,  2.66849518e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -6.28730536e-01],\n",
       "       [ 1.15264893e-01, -3.99815321e+00,  6.50429279e-02,\n",
       "         9.99864578e-01,  1.43366382e-02, -9.88920074e-05,\n",
       "         8.09142832e-03, -3.02233994e-01,  5.76752983e-03,\n",
       "         5.68266749e-01,  2.28431726e+00,  5.19047566e-02,\n",
       "         1.21439254e+00,  2.64320374e-02, -9.99172878e+00,\n",
       "         2.08674911e-02,  9.99455392e-01, -2.91337091e-02,\n",
       "        -9.53649986e-04, -1.54702496e-02,  5.77700436e-01,\n",
       "         5.45753725e-02, -1.15462375e+00, -1.15609837e+00,\n",
       "         3.21665913e-01, -4.42724556e-01, -4.57707214e+00,\n",
       "        -1.00000000e+00, -6.56128120e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  9.74787235e-01],\n",
       "       [-3.49826813e-02, -3.99904394e+00, -8.79268646e-02,\n",
       "         9.99930799e-01, -4.35103616e-03, -4.26681945e-05,\n",
       "        -1.09366644e-02,  4.28595126e-01,  1.53465057e-03,\n",
       "        -1.15287080e-01, -4.63496625e-01,  2.59015057e-02,\n",
       "        -1.72276592e+00,  7.07168579e-02, -9.99379826e+00,\n",
       "        -2.60238647e-02,  9.99532402e-01,  2.19616368e-02,\n",
       "        -6.68625347e-04,  2.12654322e-02, -8.47021699e-01,\n",
       "         3.06504928e-02,  7.00001538e-01,  2.31041408e+00,\n",
       "         2.35213175e-01,  7.58340478e-01, -5.22450447e+00,\n",
       "        -1.00000000e+00,  6.05842781e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  5.67709208e-02],\n",
       "       [ 1.48485184e-01, -3.99652076e+00, -9.73836407e-02,\n",
       "         9.99755979e-01,  1.84687823e-02,  2.01053917e-04,\n",
       "        -1.21170608e-02,  3.48167151e-01, -1.14129363e-02,\n",
       "         6.59220040e-01,  2.64991522e+00,  8.72576684e-02,\n",
       "        -1.39804232e+00,  7.45277405e-02, -9.98256683e+00,\n",
       "         4.65690494e-02,  9.98869479e-01, -3.07374895e-02,\n",
       "         2.56392173e-03,  3.61732170e-02, -1.15369260e+00,\n",
       "        -1.32114500e-01, -1.05508125e+00, -9.39309895e-02,\n",
       "         5.95965862e-01,  2.66568732e+00, -4.89613342e+00,\n",
       "        -1.00000000e+00,  6.32675791e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  3.16133469e-01],\n",
       "       [-2.55260468e-01, -3.98916936e+00, -1.77471161e-01,\n",
       "         9.99251246e-01, -3.17511931e-02, -6.32330659e-04,\n",
       "        -2.20998991e-02,  6.98441148e-01,  3.10294181e-02,\n",
       "        -1.02018440e+00, -4.10012960e+00,  2.62895137e-01,\n",
       "        -2.79904532e+00, -4.54921722e-02, -9.94654846e+00,\n",
       "        -2.24304199e-02,  9.96589422e-01,  6.65437654e-02,\n",
       "        -6.91554835e-03,  4.83079329e-02, -1.55660880e+00,\n",
       "         3.37760150e-01,  2.12730145e+00,  2.36717176e+00,\n",
       "         1.77624094e+00,  1.87421620e+00,  6.41633797e+00,\n",
       "        -1.00000000e+00, -4.77813721e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  4.18611974e-01],\n",
       "       [-1.91839218e-01, -3.99611211e+00,  1.12419128e-02,\n",
       "         9.99714255e-01, -2.38633975e-02,  2.28311601e-05,\n",
       "         1.39868539e-03, -1.13182813e-01, -4.95111453e-04,\n",
       "        -9.33291793e-01, -3.75062799e+00,  1.09465905e-01,\n",
       "         4.54790801e-01, -6.50653839e-02, -9.98377132e+00,\n",
       "         3.09333801e-02,  9.98985529e-01,  4.49926369e-02,\n",
       "        -1.30937158e-04,  1.89160951e-03,  5.68082817e-02,\n",
       "         7.79490918e-03,  1.75424516e+00,  1.25666082e+00,\n",
       "         6.19686604e-01,  5.39464355e-01, -1.54370308e+00,\n",
       "        -1.00000000e+00, -7.84964752e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -7.18573332e-01],\n",
       "       [-6.87904358e-02, -3.99933648e+00, -3.65333557e-02,\n",
       "         9.99953091e-01, -8.55602324e-03, -3.45137887e-05,\n",
       "        -4.54454869e-03,  1.44385308e-01,  1.88283331e-03,\n",
       "        -3.03365827e-01, -1.21949792e+00,  1.70748364e-02,\n",
       "        -5.80307364e-01, -6.08253479e-02, -9.99606133e+00,\n",
       "         6.52275085e-02,  9.99719739e-01,  9.87580325e-03,\n",
       "        -5.86148526e-04,  2.15108935e-02, -7.49638617e-01,\n",
       "         2.86990181e-02,  2.98682094e-01, -7.95998037e-01,\n",
       "         1.41787738e-01,  2.32287121e+00,  5.96555710e+00,\n",
       "        -1.00000000e+00, -5.33030319e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.95290446e-01],\n",
       "       [ 1.08165741e-01, -3.99694562e+00,  1.31856918e-01,\n",
       "         9.99774992e-01,  1.34523856e-02, -1.86421414e-04,\n",
       "         1.64035428e-02, -6.54324114e-01,  1.03856595e-02,\n",
       "         5.25810897e-01,  2.11405873e+00,  8.66264552e-02,\n",
       "         2.62904358e+00,  4.95681763e-02, -9.98787975e+00,\n",
       "         4.70199585e-02,  9.99262393e-01, -2.31893398e-02,\n",
       "        -1.66360009e-03, -3.05643454e-02,  1.22139835e+00,\n",
       "         9.38607007e-02, -9.02176559e-01, -3.39516371e-01,\n",
       "         4.65266198e-01, -8.33928704e-01,  5.04343796e+00,\n",
       "        -1.00000000e+00, -6.20996952e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  2.29530931e-01],\n",
       "       [-2.01361179e-01, -3.99533749e+00, -4.23126221e-02,\n",
       "         9.99672413e-01, -2.50487253e-02, -1.22899379e-04,\n",
       "        -5.26613370e-03,  1.05771072e-01,  7.28984922e-03,\n",
       "        -8.64393890e-01, -3.47346973e+00,  1.17825627e-01,\n",
       "        -4.24035460e-01, -4.43838835e-02, -9.97871876e+00,\n",
       "        -4.00466919e-02,  9.98670995e-01,  5.12206405e-02,\n",
       "        -7.85516633e-04,  5.67558687e-03, -1.48290526e-02,\n",
       "         4.06897292e-02,  1.77884579e+00,  1.84626591e+00,\n",
       "         7.37286627e-01, -6.86651468e-01,  7.91339540e+00,\n",
       "        -1.00000000e+00,  1.17395782e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -8.59579086e-01],\n",
       "       [-2.24421155e-02, -3.99951506e+00,  6.38561249e-02,\n",
       "         9.99964595e-01, -2.79123802e-03,  1.91902036e-05,\n",
       "         7.94246141e-03, -3.09393167e-01, -9.35001241e-04,\n",
       "        -9.63929147e-02, -3.87519985e-01,  1.34804212e-02,\n",
       "         1.24369514e+00,  8.62234086e-03, -9.99775505e+00,\n",
       "         1.96208954e-02,  9.99850988e-01,  7.96676986e-03,\n",
       "         2.17788052e-04, -1.53159574e-02,  5.95093310e-01,\n",
       "        -1.14169149e-02,  2.85457939e-01,  5.95614970e-01,\n",
       "         8.62112492e-02, -4.78092551e-01, -7.99777699e+00,\n",
       "        -1.00000000e+00, -1.88577652e-01,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -5.87672472e-01],\n",
       "       [ 9.13047791e-02, -3.99895453e+00, -3.97650152e-02,\n",
       "         9.99923289e-01,  1.13565288e-02,  4.74387125e-05,\n",
       "        -4.94644884e-03,  1.98310852e-01, -2.61820364e-03,\n",
       "         4.41442698e-01,  1.77450776e+00,  2.93796752e-02,\n",
       "        -7.96994150e-01,  2.95906067e-02, -9.99530983e+00,\n",
       "         4.13151830e-03,  9.99690473e-01, -2.16381587e-02,\n",
       "         5.53390360e-04,  1.22695323e-02, -4.92654294e-01,\n",
       "        -3.11391670e-02, -8.41676414e-01, -6.51357651e-01,\n",
       "         1.82749867e-01,  7.98409164e-01, -9.17732239e-01,\n",
       "        -1.00000000e+00,  7.94718599e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -3.96821141e-01],\n",
       "       [-9.74521637e-02, -3.99897003e+00, -1.84326172e-02,\n",
       "         9.99923944e-01, -1.21212276e-02, -2.35793323e-05,\n",
       "        -2.29267078e-03,  8.74133185e-02,  1.37169845e-03,\n",
       "        -4.83000994e-01, -1.94151831e+00,  2.93274317e-02,\n",
       "        -3.51291865e-01, -2.71472931e-02, -9.99527931e+00,\n",
       "         1.74713135e-02,  9.99681532e-01,  2.38365736e-02,\n",
       "        -3.81632766e-04,  8.28212686e-03, -3.22082490e-01,\n",
       "         2.19155811e-02,  9.50845420e-01,  8.44808280e-01,\n",
       "         1.87192693e-01,  8.18017900e-01, -7.58142281e+00,\n",
       "        -1.00000000e+00, -2.55382538e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  5.95939159e-01],\n",
       "       [-3.04260254e-02, -3.99938846e+00,  6.90345764e-02,\n",
       "         9.99955952e-01, -3.78414337e-03,  2.85504793e-05,\n",
       "         8.58679507e-03, -3.24335337e-01, -1.30064297e-03,\n",
       "        -1.21671028e-01, -4.89146352e-01,  1.64494496e-02,\n",
       "         1.30372906e+00, -6.40258789e-02, -9.99743366e+00,\n",
       "         1.29852295e-02,  9.99837637e-01, -1.81689078e-03,\n",
       "         1.50003092e-04, -1.79274678e-02,  7.02593088e-01,\n",
       "        -7.19017163e-03, -9.39959437e-02, -1.27211475e+00,\n",
       "         9.77195874e-02, -8.52607429e-01,  6.22556686e+00,\n",
       "        -1.00000000e+00, -5.02416992e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00, -5.23933172e-01],\n",
       "       [ 6.43127412e-02, -3.99951148e+00, -1.76239014e-02,\n",
       "         9.99965668e-01,  7.99921993e-03,  1.50827618e-05,\n",
       "        -2.19214195e-03,  9.15835723e-02, -6.86718908e-04,\n",
       "         2.68670201e-01,  1.08000863e+00,  1.24236848e-02,\n",
       "        -3.68118703e-01, -3.31174508e-02, -9.99548435e+00,\n",
       "         4.74891663e-02,  9.99621034e-01, -2.42356956e-02,\n",
       "         4.86830424e-04,  1.30525865e-02, -5.28743386e-01,\n",
       "        -2.46203672e-02, -8.43647599e-01, -1.87196326e+00,\n",
       "         1.77095503e-01,  1.70786619e+00,  3.21875191e+00,\n",
       "        -1.00000000e+00, -7.32390976e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  5.68796635e-01],\n",
       "       [ 8.95366669e-02, -3.99909496e+00, -2.58636475e-02,\n",
       "         9.99932826e-01,  1.11365812e-02,  2.99334024e-05,\n",
       "        -3.21737514e-03,  1.31878614e-01, -1.70413603e-03,\n",
       "         4.48248774e-01,  1.80185759e+00,  2.60039736e-02,\n",
       "        -5.30022979e-01,  7.69958496e-02, -9.99610233e+00,\n",
       "         6.43615723e-02,  9.99745667e-01, -1.32201500e-02,\n",
       "         6.00025931e-04,  1.82603635e-02, -7.13272691e-01,\n",
       "        -3.54243070e-02, -5.57783723e-01,  6.58943236e-01,\n",
       "         1.54153794e-01,  2.24852824e+00, -2.27559662e+00,\n",
       "        -1.00000000e+00, -7.66952896e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.31489396e-01]]),), done=(array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]]),))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = agent.memory.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the agent\n",
    "\n",
    "Now run DQN learning algorithm and display average score over episodes benchmark. In order to solve the environment, the agents must get an average score of +30 (over 100 consecutive episodes, and over all agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.01\n",
      "Learning took: 1008.1557946205139, per episode: 10.08155794620514\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def a2c(n_episodes=100, max_t=1000, solved_score=30):\n",
    "    \"\"\"Advantage Actor Critic (synchronous).\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        score = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)                    # select an action\n",
    "            actions_clamped = torch.clamp(actions, -1, 1).cpu().numpy()\n",
    "            env_info = env.step(actions_clamped)[brain_name]       # send the action to the environment\n",
    "            next_states = env_info.vector_observations     # get the next state\n",
    "            rewards = env_info.rewards                     # get the reward\n",
    "            dones = env_info.local_done                    # see if episode has finished\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        agent.learn()\n",
    "        episode_score = np.mean(score)\n",
    "        scores_window.append(episode_score)       # save most recent score\n",
    "        scores.append(episode_score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=solved_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.model_local.state_dict(), 'trained_weights.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "start = time.time()\n",
    "scores = a2c()\n",
    "learning_took = time.time() - start\n",
    "\n",
    "print(\"Learning took: {}, per episode: {}\".format(learning_took, learning_took/len(scores)))\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the agent\n",
    "\n",
    "Perform a test run for a single episode and display the total score of untrained agent. Score over 13 is considered success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=100, fc2_units=100):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=100, fc2_units=100):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        xs = self.bn1(xs)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.soft_update(self.actor_local, self.actor_target, 1.0)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.soft_update(self.critic_local, self.critic_target, 1.0)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, np.expand_dims(reward, 1), next_state, np.expand_dims(done, 1))\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size // 20)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations\n",
    "action = agent.act(state)\n",
    "env_info = env.step(action)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 21.73\n",
      "Episode 126\tAverage Score: 30.23\n",
      "Environment solved in 26 episodes!\tAverage Score: 30.23\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ddpg(n_episodes=1000, max_t=1000, solved_score=30):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        agent.reset()\n",
    "        states = env_info.vector_observations\n",
    "        score = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)       # select an action\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]       # send the action to the environment\n",
    "            next_states = env_info.vector_observations     # get the next state\n",
    "            rewards = env_info.rewards                     # get the reward\n",
    "            dones = env_info.local_done                    # see if episode has finished\n",
    "            # for i in range(num_agents):\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        episode_score = np.mean(score)\n",
    "        scores_window.append(episode_score)       # save most recent score\n",
    "        scores.append(episode_score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=solved_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 39.369999120011926, steps: 999\n"
     ]
    }
   ],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations\n",
    "score = np.zeros(num_agents)\n",
    "for j in range(1000):\n",
    "    action = agent.act(state, add_noise=False)     # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations      # get the next state\n",
    "    reward = env_info.rewards                      # get the reward\n",
    "    done = env_info.local_done                     # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if np.any(done):                               # exit loop if episode finished\n",
    "        break\n",
    "\n",
    "print(\"Score: {}, steps: {}\".format(np.mean(score), j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close\n",
    "\n",
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7042e7523171f2a1e37a031fc61f6d1b03fe8cfad426cc0438c624b42620eab4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('drlnd': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
